{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c2d3b0",
   "metadata": {},
   "source": [
    "# oveview\n",
    "> svm 需要解决以下四个问题\n",
    "1. 对于分类问题硬边界优化函数的建立、对偶问题的建立、模型的求解\n",
    "2. 软边界的加入，损失函数以及简单化处理，模型的求解\n",
    "3. 核函数的加入，模型的求解\n",
    "4. 对于拟合问题优化函数的建立、对偶问题的建立、模型的求解\n",
    "\n",
    "> svm 需要理解的几个重要超参数\n",
    "1. 软边界中的正则化常数$C$ 以及模型超参数中未出现的松弛变量$\\xi$\n",
    "2. 核函数名称，核函数维度\n",
    "3. 对于SVR而言，还要确定边界参数$\\epsilon$\n",
    "\n",
    "> svm 注意事项\n",
    "1. 不适合有部分残缺的数据集\n",
    "2. 过度升维可能会导致过拟合现象\n",
    "3. 极度依赖数据本身的范围，使用前最好进行标准化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90715948",
   "metadata": {},
   "source": [
    "# hard-margin model\n",
    "## mathmatical background\n",
    "> special analytic geometry\n",
    "$$\n",
    "    d = \\frac{|\\omega^T \\cdot x + b|}{||\\omega||}\n",
    "$$\n",
    "\n",
    "> to make sure each data point are assigned to correct class, label and predict value should be of the same sign\n",
    "\n",
    "$$\n",
    "    y_i(\\omega^T \\cdot x_i + b) \\ge 1\n",
    "$$\n",
    "\n",
    "> where 1 here is set as hard-margin, which is for the sake of calculation simplicity\n",
    "> here, we assume $\\omega^T \\cdot x + b$ set to be 1, distance between to support vectors should be:\n",
    "\n",
    "$$\n",
    "    \\frac{2}{||\\omega||}\n",
    "$$\n",
    "\n",
    "> then optimization problem is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathop{\\arg\\max}\\limits_{\\omega}  & \\frac{2}{||\\omega||} \\\\\n",
    "s.t. & y_i(\\omega^T \\cdot x_i + b) \\ge 1 \\space,\\space i = 1,2,\\dots n\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "> which could be transferred to \n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathop{\\arg\\min}\\limits_{\\omega}  & \\frac{1}{2}{||\\omega||}^2 \\\\\n",
    "s.t. & y_i(\\omega^T \\cdot x_i + b) \\ge 1 \\space,\\space i = 1,2,\\dots n\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbbca4a",
   "metadata": {},
   "source": [
    "## mathmatical derivation\n",
    "> Lagrange multiplier method\n",
    "\n",
    "$$\n",
    "    L(\\omega, b, \\alpha) = \\frac{1}{2}{||\\omega||}^2 + \\sum_{i=1}^{n}{\\alpha_i[1-y_i(\\omega^T \\cdot x + b)]}\n",
    "$$\n",
    "\n",
    "> kkt condition:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\alpha_i & \\ge & 0 \\\\\n",
    "y_i(\\omega^T \\cdot x + b) - 1 & \\ge & 0 \\\\\n",
    "\\alpha_i \\cdot [y_i(\\omega^T \\cdot x + b) - 1] & = & 0\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "> duel problem, note that the upper bound of the optimization result of duel problem is the lower bound of the original problem\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathop{\\arg\\max}\\limits_{\\alpha}  & \\displaystyle\\sum_{i}^{n}{\\alpha_i} - \\frac{1}{2} \\displaystyle\\sum_{i}^{n}\\displaystyle\\sum_{j}^{n}{\\alpha_i\\alpha_jy_iy_j{x_i}^Tx_j} \\\\\n",
    "s.t. & \\displaystyle\\sum_{i}^{n}{\\alpha_iy_i} = 0,\\space i = 1,2,\\dots n \\\\\n",
    "& \\alpha_i \\ge 0,\\space i = 1,2,\\dots n \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb1b89",
   "metadata": {},
   "source": [
    "# soft-margin model\n",
    "> adding penalty term punishing data points that are off-the-street of the right side of classification plane\n",
    "> note that penalty term contains two main parts: \n",
    "1. regularization constant, in svm case this parameter is $C$, for larger $C$, soft-margin model will perform like one hard-margin model\n",
    "2. cost function, of which independent variable is set to be some index of model, in svm case is the distance $y_i(\\omega^T \\cdot x_i + b)$\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathop{\\arg\\min}\\limits_{\\omega} \\frac{1}{2}{||\\omega||}^2 + C \\cdot \\displaystyle\\sum_{i}^{n}{l_{1/0}[1-y_i(\\omega^T \\cdot x_i + b)]}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "> where $l_{1/0}$ is one cost function, this function is related to parameter $\\omega$ and $b$. for the sake of calculation simplicity, we introduce slack variables in replace of cost function, which could be \n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathop{\\arg\\min}\\limits_{\\omega,\\xi_i} & \\frac{1}{2}{||\\omega||}^2 + C \\cdot \\displaystyle\\sum_{i}^{n}{\\xi_i} \\\\\n",
    "s.t. & y_i[(\\omega^T \\cdot x_i + b)] \\ge 1- \\xi_i\\\\\n",
    "& \\xi_i \\ge 0 \\space , \\space i = 1,2, \\dots, n\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "> where $\\xi_i$ could possibily be greater than 1. in this case, data point would cross the classification plane and locate on the other side of plane. comparing to hard-margin restriction condition $y_i[(\\omega^T \\cdot x_i + b)] \\ge 1$, this condition $y_i[(\\omega^T \\cdot x_i + b)] \\ge 1- \\xi_i$ takes far-off-street points into consideratioin\n",
    "\n",
    "> in this case, $\\xi_i$ is the replacement of cost function, note that cost function is also the function of $\\omega$ thus equation simplification as well as duel problem making should take $\\xi_i$ into consideration\n",
    "\n",
    "> lagrange multiplier method:\n",
    "\n",
    "$$\n",
    "   L(\\omega, b, \\alpha, \\xi, \\mu) = \\frac{1}{2}{||\\omega||}^2 + \\sum_{i=1}^{n}{\\alpha_i[1 - \\xi_i - y_i(\\omega^T \\cdot x + b)]} - \\displaystyle\\sum_{i=1}^{n}{\\mu_i\\xi_i}\n",
    "$$\n",
    "\n",
    "> corresponding kkt condition:\n",
    "\n",
    "$$\n",
    "\\left \\{\n",
    "\\begin{array}{ll}\n",
    "\\alpha_i & \\ge & 0 \\\\\n",
    "y_i[(\\omega^T \\cdot x_i + b)] - 1 + \\xi_i & \\ge & 0 \\\\\n",
    "\\alpha_i \\cdot [y_i(\\omega^T \\cdot x_i + b) - 1 + \\xi_i] & = & 0 \\\\\n",
    "\\xi_i & \\ge & 0 \\\\\n",
    "\\mu_i & \\ge & 0 \\\\\n",
    "\\mu_i\\xi_i & = & 0\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "> dual problem is like:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathop{\\arg\\max}\\limits_{\\alpha}  & \\displaystyle\\sum_{i}^{n}{\\alpha_i} - \\frac{1}{2} \\displaystyle\\sum_{i}^{n}\\displaystyle\\sum_{j}^{n}{\\alpha_i\\alpha_jy_iy_j{x_i}^Tx_j} \\\\\n",
    "s.t. & \\displaystyle\\sum_{i}^{n}{\\alpha_iy_i} = 0,\\space i = 1,2,\\dots n \\\\\n",
    "& 0 \\le \\alpha_i \\le C ,\\space i = 1,2,\\dots n \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5e78a",
   "metadata": {},
   "source": [
    "# kernal function\n",
    "> raise dimensionality, yet the inner product result of a high dimensional space can be represented by a vector of the original space\n",
    "> kernal function are delicately designed to meet the aforementioned condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d2193",
   "metadata": {},
   "source": [
    "# SVR\n",
    "> different from SVC-SVM, regression only takes predictions that are largely diviating from target into consideration when calculating cost function(optimization problem). This margin is set as $\\epsilon$\n",
    "> thus, we take this cost function into practice\\\n",
    "\n",
    "$$\n",
    "    l_{\\epsilon}(z) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "0& , & if |z| \\le \\epsilon \\\\\n",
    "|z| - \\epsilon& , & otherwise\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "> then optimization problem becomes:\n",
    "\n",
    "$$\n",
    "    \\mathop{\\arg\\min}\\limits_{\\omega} \\frac{1}{2}{||\\omega||}^2 + C \\cdot \\displaystyle\\sum_{i}^{n}{l_{\\epsilon}[(\\omega^T \\cdot x_i + b)-y_i]}\n",
    "$$\n",
    "\n",
    "> introduce slack variable into model:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathop{\\arg\\min}\\limits_{\\omega,\\xi_i, \\hat\\xi_i} & \\frac{1}{2}{||\\omega||}^2 + C \\cdot \\displaystyle\\sum_{i}^{n}{\\xi_i + \\hat\\xi_i} \\\\\n",
    "s.t.  \n",
    "      & (\\omega^T \\cdot x_i + b)-y_i \\le \\epsilon + \\xi_i \\space,\\\\\n",
    "      & y_i - (\\omega^T \\cdot x_i + b) \\le \\epsilon + \\hat\\xi_i \\space,\\\\\n",
    "      & \\xi_i \\ge 0 \\space,\\\\\n",
    "      &\\hat\\xi_i \\ge 0 \\space, i= 1,|2,\\dots n\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "> in this case, we just consider $l_{\\epsilon}[(\\omega^T \\cdot x_i + b)-y_i]$ as two inequality constrains\n",
    "> thus duel problem is like:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "     \\mathop{\\arg\\max}\\limits_{\\alpha,\\hat\\alpha} & \\sum_{i=1}^{n}{y_i(\\hat\\alpha_i - \\alpha_i)-\\epsilon(\\hat\\alpha_i - \\alpha_i)} - \\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n}{(\\hat\\alpha_i - \\alpha_i)(\\hat\\alpha_j - \\alpha_j){x_i}^Tx_j} \\\\\n",
    "    s.t. & \\sum_{i=1}^{n}{(\\hat\\alpha_i - \\alpha_i)=0} \\\\\n",
    "    & 0 \\le \\alpha_i , \\hat\\alpha_i \\le C\n",
    "\\end{array}\n",
    "$$ \n",
    "\n",
    "> the corresponding kkt conditions are:\n",
    "\n",
    "$$\n",
    "\\left \\{\n",
    "\\begin{array}{ll}\n",
    "    & \\alpha_i(\\omega^T \\cdot x_i + b -y_i - \\epsilon - \\xi_i) = 0 \\\\\n",
    "    & \\hat\\alpha_i(\\omega^T \\cdot x_i + b -y_i - \\epsilon - \\hat\\xi_i) = 0 \\\\\n",
    "    & \\alpha_i \\hat\\alpha_i = 0 \\\\\n",
    "    & \\xi_i\\hat\\xi_i = 0 \\\\\n",
    "    & (C - \\alpha_i)\\xi_i = 0 \\\\\n",
    "    & (C - \\hat\\alpha_i)\\hat\\xi_i = 0 \\\\\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "> as results, we could calculate each $\\mathbf{\\omega}$ with $\\alpha, \\hat\\alpha$:\n",
    "\n",
    "$$\n",
    "    \\mathbf{\\omega} = \\sum_{i=1}^{n}{(\\hat\\alpha_i - \\alpha_i)\\mathbf{x_i}}\n",
    "$$\n",
    "\n",
    "> for scalar $b$, we could find several $\\mathbf{x}$, then calculate the mean value for $b$ according to this formula:\n",
    "\n",
    "$$\n",
    "    b = y_i + \\epsilon - \\sum_{j=1}^{n}{(\\hat\\alpha_j - \\alpha_j)\\mathbf{x_j}^T \\mathbf{x_i}}\n",
    "$$\n",
    "\n",
    "> * where $\\mathbf{x_j}$ is the data points who meet $0<\\alpha_i<C$ condition, and $\\mathbf{x_i}$ are a ramdom data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aaabd2",
   "metadata": {},
   "source": [
    "## sklearn SVC\n",
    "1. important hyperparameters are:\n",
    "    > kernel <br>\n",
    "    > degree <br>\n",
    "    > C <br>\n",
    "    > coef0 and gamma are for 'ploy' 'sigmoid' and 'rbf' kernel <br>\n",
    "    > decision_function_shape: 'ovo' or 'ovr'(default) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46f9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e6aa18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
